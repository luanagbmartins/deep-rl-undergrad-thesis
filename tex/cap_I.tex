\chapter{Introdução}
\label{cap:intro}

A capacidade de aprender é um aspecto bastante importante da inteligência artificial. É impossível especificar todo o conhecimento que um agente inteligente precisará em tarefas não triviais do mundo real, portanto o agente precisa ter a capacidade de aprender a lidar com essa falta de conhecimento. Aprendizado por reforço, tendo suas raízes na psicologia comportamental (o behaviorismo), é uma abordagem baseada na ideia de que comportamentos são aprendidos através interação com o ambiente \cite{krapfl}. De acordo com o behaviorismo, o comportamento pode ser estudado de maneira sistemática e observável, independentemente dos estados mentais internos \cite{abramson}.

O aprendizado por reforço é um ramo de aprendizado de máquina, em que um agente aprende como se comportar em um ambiente executando ações e analisando observações e resultados obtidos. É um método de programação de agentes através do oferecimento de recompensas e punições, sem a necessidade de especificar como uma tarefa deve ser realizada, cujo objetivo é maximizar a recompensa esperada. Pensando dessa maneira, jogos podem ser facilmente modelados como um ambiente em uma configuração de aprendizado por reforço, em que agentes (jogadores) têm um conjunto finito de ações que podem ser executadas em cada etapa e sua sequência de movimentos determina seu sucesso. 

Desde o nascimento da ideia de inteligência artificial jogos tem sido uma maneira eficiente para medir a sua capacidade, tornando a prática de avaliação de algoritmos usando jogos e competições comum entre pesquisadores. Os jogos fornecem referências parametrizáveis que permitem experimentação rápida com várias abordagens, enquanto as competições estabelecem uma estrutura e um conjunto de regras comuns para garantir que esses algoritmos sejam comparados de maneira justa. Treinar um agente para superar os jogadores humanos \cite{silver16} e otimizar sua pontuação pode nos ensinar como otimizar processos diferentes em uma variedade de subcampos diferentes. 

Recentemente, o aprendizado por reforço tem se mostrado muito bem-sucedido em problemas complexos de alta dimensão, em grande parte devido ao aumento da potência computacional e ao uso de redes neurais profundas para aproximação de funções \cite{mnih13, mnih16, lillicrap15}. Embora tenham sido feitos grandes avanços no desenvolvimento de algoritmos que aprendem com eficiência tipos específicos de problemas, agentes inteligentes também deveriam ser capazes de generalizar tarefas, usando a experiência anterior para adquirir novas habilidades mais rapidamente. Entretanto, a generalização entre tarefas permanece difícil para os algoritmos de aprendizado de reforço profundo de última geração, e os ambientes de avaliação mais comuns de aprendizado por reforço ainda incentivam o treinamento e avaliação no mesmo conjunto de ambientes. Buscando avançar os estudos nessa área, foram criados alguns ambientes de jogos eletrônicos com foco em facilitar o desenvolvimento, o teste e a avaliação de algoritmos de aprendizado por reforço com esse tipo de capacidade, separando explicitamente os ambientes de treinamento e teste \cite{nichol18, Juliani19, torrado18}.

Este trabalho tem como objetivo a análise da capacidade de generalização do algoritmo de aprendizado por reforço \textit{Proximal Policy Optimization}, que demonstrou ser aplicável em configurações mais gerais e ter melhor desempenho geral \cite{Schulman17}. Com base nisso, este documento está organizado em cinco capítulos. No Capítulo \ref{cap:fundamento} há uma descrição dos conceitos e técnicas utilizadas, incluindo redes neurais, aprendizado de máquina, aprendizado por reforço e por fim a técnica de aprendizado de reforço utilizado neste trabalho. Em seguida, o Capítulo \ref{cap:metodologia} apresenta a explicação dos materiais e métodos utilizados, incluindo os ambientes de jogos eletrônicos e uma descrição dos testes realizados. No Capítulo \ref{cap:resultados} estão descritos em detalhes os resultados obtidos, e, por fim no Capítulo \ref{cap:conclusao} apresentam-se as conclusões e trabalhos futuros.
