\chaves{Inteligência Artificial; Aprendizado de Máquina; Aprendizado por Reforço; Redes Neurais; Agentes Inteligentes; Jogos; GVGAI; Proximal Policy Optimization; PPO}
\begin{resumo} 
Em grandes problemas, os sistemas de aprendizado por reforço devem utilizar aproximadores de função parametrizados, como redes neurais, para generalizar entre situações e ações semelhantes. Apesar de ter se mostrado um método eficaz em problemas complexos específicos, ainda falham no aspecto de generalização, e os ambientes de avaliação mais comuns de aprendizado por reforço ainda incentivam o treinamento e avaliação no mesmo conjunto de ambientes. Para ser possível avaliar a capacidade de um algoritmo generalizar entre tarefas é necessário ambientes de avaliação que medem o seu desempenho em um conjunto de testes distintos daqueles utilizados no treinamento. Com isso, este trabalho se propõe a avaliar o desempenho do algoritmo \textit{Proximal Policy Optimization} (PPO), no ambiente de avaliação \textit{General Video Game Artificial Intelligence} (GVGAI) que fornece uma subdivisão de um mundo virtual de um jogo em diferentes fases ou níveis. Apesar do PPO em geral reportar ótimos resultados, é possível notar que o algoritmo sofre com sobre-ajuste ao conjunto de treinamento.
\end{resumo}

